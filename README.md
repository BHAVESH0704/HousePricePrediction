I started with almost no ML knowledge, just basic Python and curiosity.

At first, I learned what machine learning really means — teaching a computer using examples. I built my first model using Linear Regression and predicted house prices based on area. It felt simple, but it was my first real win.

Then I moved to real ML practices:

Split data into training and testing

Learned why models shouldn’t memorize data

Measured mistakes using MSE

Next, I stepped into the real world with Kaggle data. The dataset was bigger, messier, and more realistic. I learned how to clean data, choose useful features, and handle mistakes without panicking.

When accuracy wasn’t good, I didn’t quit — I improved it:

Added better features

Understood when scaling helps and when it doesn’t

Switched to Random Forest, which was a big upgrade

I faced confusing errors, warnings, even negative R² scores — but I learned that debugging is part of ML, not a failure.

Finally, I pushed my project to GitHub, learned Git properly, fixed real push errors, and made my work public.
